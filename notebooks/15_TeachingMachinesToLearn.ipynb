{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teaching Machines to Learn\n",
    "\n",
    "<img src=https://einstein.ai/static/images/pages/einstein-scroll.png width=300>\n",
    "\n",
    "<br>\n",
    "\n",
    "*The first part of these notes were generated largely by Suman Deb Roy, with small edits by Mark Hansen.* \n",
    "\n",
    "**A World of Signals**\n",
    "\n",
    "Perception, whether human or machine, is fundamentally dependent on our ability to *sense and analyze signals* that abound in the natural and digital worlds. Humans are evolutionarily powered to sense these, although variations in ability exists from person to person. But machines have to be taught first what perception means, and then taught how to keep learning on acquired perception principles. \n",
    "\n",
    "This task makes for some of the most fascinating and interesting challenges that exist in our quest to make machines smarter. It also automatically divides computing into many subfields, based on the signals that a machine will encounter and must analyze:  \n",
    "\n",
    "- When the signal is image/video/gif : *Computer Vision*\n",
    "- When the signal is text            : *Natural Language Processing*\n",
    "- When the signal is touch           : *Haptic Computing*\n",
    "- When the signal is sound           : *Speech Recognition / Audio Signal Processing*\n",
    "- When the signal is smell           : *There's [Cyranose!](https://en.wikipedia.org/wiki/Electronic_nose) / Classification of foods, bacteria detection [\\(you laugh\\)](http://www.disi.unige.it/person/MasulliF/papers/masulli-mcs02.pdf)*\n",
    "- When the signal is taste           : *We aren't there yet* (although... [http://fastml.com/predicting-wine-quality/](Principal Component Analysis for wine quality))\n",
    "\n",
    "The idea is that if machines can analyze these signals (themselves representations of the signals we as humans perceive) accurately, they will have intelligence in dealing with situations that humans have to deal with on a daily basis. For example, could a machine analyze an image and recognize smiling faces? Could it analyze text and recognize abusive language? Can it analyze speech tones and recognizes distress or strain? Can it sense the pressure of a touch, and associate the magnitude of pressure with different intentions of the user. As you can see, all of these so called \"learnings\" are trying to make machines perceive and analyze signals like humans do.\n",
    "\n",
    "As we have seen from the case of Cambridge Analytica, we are immediately confronted with important questions as these activities take on social, political or even cultural importance. The idea that a machine is making decisions is often confused with objectivity in decision making, for example. By talking through the Machine Learning \"pipeline,\" we can highlight where human decision making is required to design and build a learning system. These are places where our biases come into play. We will consider very different situations, and highlight the various reporting possibilities along each pipeline, from signals to actions.\n",
    "\n",
    "We will see that Machine Learning procedures means different things to human learning. Sometimes, by creating a Machine Learning model, we learn about how something in the world works -- like mathematical models in physics. Sometimes, we want to the model to simply drive the car and we don't really care how it does it. We just need it to be safe. The roles of narrative and human understanding are often left out when we talk about ML. (More on this shortly.)\n",
    "\n",
    "A couple good readings to help you think through these roles are [A Practice-Based Framework for Improving Critical Data Studies and Data Science](https://www.liebertpub.com/doi/pdf/10.1089/big.2016.0050) and [Feminist Data Visualization](http://www.kanarinka.com/wp-content/uploads/2015/07/IEEE_Feminist_Data_Visualization.pdf). \n",
    "\n",
    "**AI and Machine Learning**\n",
    "\n",
    "Originally there were three subdivisions of AI: (1) Neural Networks, (2) Genetic Programming (Evolutionary Computing) and (3) Fuzzy Systems. As data became abundantly available and computation became cheaper and more powerful, a more statistical approach came into the forefront. This was when machine learning was born. You will see the terms 'AI' and 'Machine Learning' interchanged often. Machine Learning is a *type* of AI that is heavily dependent on data.\n",
    "\n",
    "- Machine Learning: The ability of computers to learn from data without an **explicitly** pre-programmed rule set.\n",
    "- Artificial Intelligence: Intelligence exhibited by machines. \n",
    "\n",
    "\n",
    "\n",
    "| Method | Learning Model | Improvement Criteria | ~Year | Pitfalls | \n",
    "| ------ | ----------- | ----------- | ----------- | ----------- | \n",
    "|1. Old AI   |  Precoded Rules | X | 1950s | Too few rules |\n",
    "|2. Expert Systems | Inferred Rules | X | 1970s | Knowledge Acquisition problem |\n",
    "|3. AI Winter | :( | :( | 1980s |  :( |\n",
    "|4. Machine Learning | Data | Experience + Reinforcement| 1990s | Black box models |\n",
    "|5. Deep Learning | Lots of Data | Experience + Reinforcement + Memory | 2000s | Cannot explain itself (yet) |\n",
    "\n",
    "<br>\n",
    "\n",
    "**Your ML ability is limited by the data you have**\n",
    "\n",
    "There are many things you have to do before you get to the \"algorithm\" or \"modeling\" phase of a machine learning system. The chief among these is to transform/format the data so it is easily ingestable by the algorithm. You must also look if your data is biased (it will be but you have to be aware of how and whether different people, places or situations are treated in a discriminatory way). Who collected the data? What was their intention? Their motivations? Whose voices were left out? Then you must choose a type of machine learning algorithm to use. This is almost always dependent on the kind of data you have (more below). There are also so many different kinds of algorithms now, it can be hard to know why to choose one over another. Sometimes it will depend on the \"difficulty\" of your learning task, sometimes you want to be able to interpret your model - \"Why this decision for this input?\" You can always run multiple algorithms on your data set and test the outcome of which model performs better or provides better interpretation. \n",
    "\n",
    "So lets quickly list the topics that we have/will covered:\n",
    "\n",
    "> Data Ingestion\n",
    "    - Data Formats (e.g., dataframes, dictionaries) **\n",
    "    - Data Discovery **\n",
    "    - Data Acquisition **\n",
    "    - Integration and Fusion (beware of Simpson's Paradox)\n",
    "    - Transformation + Enrichment **\n",
    "    \n",
    "> Data Munging\n",
    "    - Principal Component Analysis (PCA)  ** \n",
    "    - Dimensionality Reduction **\n",
    "    - Sampling\n",
    "    - Denoise\n",
    "    - Feature Extraction\n",
    "    \n",
    "> Types\n",
    "    - Supervised (I know the label of each data instance.)\n",
    "    - Unsupervised (I do not know the label of any data instance.) \n",
    "    - SemiSupervised (some labeled, mostly unlabeled) \n",
    "    \n",
    "> Supervised Algorithms:  \n",
    "    - Decision Trees (this class)\n",
    "    - Random Forests (this class)\n",
    "    - Linear Regression (this class)\n",
    "    - Support Vector Machines (next class)\n",
    "    \n",
    "> Unsupervised Algorithms: \n",
    "    - Kmeans ** \n",
    "    - Neural Nets \n",
    "    \n",
    "** The ML pipeline... **\n",
    "\n",
    "*Here are the basic steps in building machine learning algorithm:*\n",
    "1. Signal Detection: find a source, check if it generates a signal\n",
    "2. Give values to those signals. E.g, each like button press = +1 POS vote but each heart/star press = +2 POS votes. \n",
    "3. Sample which parts of the data you want to use, or is usable.\n",
    "4. Split your data into 70%-30% between training & test. Training data is used to BUILD the model. Test data is used to EVALUATE the model. Ideally, you'd also keep some for validation, which is used to TUNE the model. \n",
    "5. Have a reinforcement framework so your model can improve over time.\n",
    "\n",
    "*Things to think about*:\n",
    "- Which nodes are most manual?\n",
    "- In which nodes can bias creep in.. and how?\n",
    "- Which nodes lead to black box?\n",
    "\n",
    "We will start with a model that is easy to interpret but often too inflexible to predict very well. It is one of the most frequently used models we have and was developed almost entirely by one man, Francis Galton.\n",
    "\n",
    "**Linear regression**\n",
    "\n",
    "[Start with PDF of background slides](https://www.dropbox.com/s/mtt4w718duwtko1/Regression.pdf?dl=0)\n",
    "\n",
    "A linear regression attempts to find the relationship between two sets of variables -- one or more \"independent variables\" or \"predictors\" and a \"dependent variable\" or \"outcome\". To start, given a data set with two columns, X and Y, its task is to find a line that best describes Y as a function of X. It is used to figure out serious things in the real world like GDP, exchange rates, money flows, etc. and is a heavily used research tool in the social and political sciences.  \n",
    "\n",
    "We will use SciKit Learn to create the fit. If you haven't yet, please install some code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sklearn --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydotplus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "height = read_csv(\"https://github.com/computationaljournalism/columbia2020/raw/master/data/galton.csv\")\n",
    "height.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can dust off our `plotly` and have a look at the marginal distributions of midparents' heights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.express import histogram\n",
    "\n",
    "fig = histogram(height, x=\"midparent\",nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare parents to children. Make a histogram of the children's heights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we plot them side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = histogram(height, x=\"child\",nbins=20)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, make a plot of all the children's heights who have midparents that are 67.5in tall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the table From Galton's paper using `crosstab()` from `pandas`. It creates a table of pairs of values. Something new! (Note that the table looks upside down because Galton ordered his rows from high to low, whereas `crosstab` orders them from low to high.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import crosstab\n",
    "\n",
    "crosstab(height[\"child\"],height[\"midparent\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can include the \"marginal\" distribution by simply asking that `margins` be included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab(height[\"midparent\"],height[\"child\"],margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot one variable against another we simply make a scatterplot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = height.groupby(height.columns.tolist()).size().reset_index()\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = counts.rename(columns={0:\"number\"})\n",
    "counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.express import scatter\n",
    "\n",
    "fig = scatter(counts, x=\"midparent\",y=\"child\",size=\"number\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a model that tries to fit this data. we start with linear regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(height[[\"midparent\"]], height[\"child\"])\n",
    "\n",
    "print(\"The intercept is\", model.intercept_)\n",
    "print(\"The list (here just one) of coefficients is\", model.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember, Galton eye-balled the slope at 2/3. Not too shabby :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add line to plot\n",
    "\n",
    "fig = scatter(counts, x=\"midparent\",y=\"child\",size=\"number\",trendline=\"ols\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course you can apply this to any data set we've seen so far in an attempt to understand something about how variables relate, perhaps how nature works. Often, the narrative content has to do with the slope - is it positive or negative? How big is it? This allows us to tell stories about how two variables relate to each other. It is possible to beef up this technique considerably and it is at the core of a lot of more advanced fitting procedures. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "\n",
    "data = read_csv(\"https://github.com/computationaljournalism/columbia2020/raw/master/data/hardest.csv\")\n",
    "data.dropna(axis=0,how=\"any\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(data[[\"education\"]], data[\"income\"])\n",
    "\n",
    "print(\"The intercept is\", model.intercept_)\n",
    "print(\"The list (here just one) of coefficients is\", model.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = scatter(data, x=\"education\",y=\"income\",trendline=\"ols\",trendline_color_override=\"black\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Two cultures**\n",
    "\n",
    "We are now going to consider a general purpose learning machine. It was developed in large part by Leo Breiman and by my advisor Charles J. Stone. [Later in his career, he was one of the first to write about algorithms for working with data,](https://projecteuclid.org/euclid.ss/1009213726) framing the discussion as a kind of disciplinary divide between traditional statistics and more modern (at the time) computer scientists -- what he refers to as the \"two cultures\" in modeling. In his opening paragraph, he nicely summarizes two goals for creating a data model.\n",
    "\n",
    "<img src=https://github.com/computationaljournalism/columbia2018/raw/master/images/lb1.jpg width=40%>\n",
    "\n",
    "Breiman argues that statistics has been consumed with what's in the box, and their tools start by tryig to explain how data are created. Tools like regression is an example (we'll come back to them). By contrast the \"new models\" he is referring to in his paper are more concerned with prediction, allowing them to be freer in their approaches to a problem. They don't prejudge what nature's up to, but instead only care about the success of the algorithm. \n",
    "\n",
    "In his paper, Brieman distills the two main properties of models as \"interpretation\" and \"prediction.\"  Simple linear regression is heavy on interpretation but light on prediction (see what happens if you try to fit a simple line to most of the data in the \"hardest\" data set. \n",
    "\n",
    "According to Breiman, the tools we are going to look at next, a class of models known as \"decision trees\" score an \"A+ for interpretation\" but, according to Breiman, just a B+ on prediction. You'll see why. Brieman introduced decision trees in statistical practice, but then extended them to create an A+ predictor out of them. We'll see!\n",
    "\n",
    "**Decision trees**\n",
    "\n",
    "A **decision tree** is statistical model that is at one time a useful visualization as well as a prediction engine. It falls under Breiman's classification as an algorithmic tool because it originated outside of statistics and does not start with a set of assumptions about how nature works. Instead, it finds associations through an algorithm. Now, it's an interesting case bacause the tool itself is fairly powerful as an explanatory engine -- its visualization carries considerable narrative content about how nature might be making decisions. This is why it scores an A+ for interpretability. \n",
    "\n",
    "So let's have a look at an example. The data comes from the election in 2008, and specifically during the democratic primary. It was developed by Amanda Cox, an outstanding visual journalist at The New York Times. I use such old data because this piece is a near perfect distillation of what I mean by the narrative content in these models.\n",
    "\n",
    "![tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)\n",
    "\n",
    "Have a look. The tree is like a game of 20 questions. If you want to know how a county will vote, answer a few questions. What is the racial makeup? What about the HS graduation rate? Where in the country is the county? \n",
    "\n",
    "The questions are in the form of a tree -- as we have seen before, computer scientists grow their trees upside down. As you answer questions, you eventually end up at a leaf and a decision is made. Note that it's not perfect -- there is some uncertainty in the decision. \n",
    "\n",
    "To show that this isn't a one-off, ProPublica had a lovely project called the [Message Machine](https://projects.propublica.org/emails/) where they looked email messages sent out by [political campaigns](https://www.propublica.org/special/message-machine-you-probably-dont-know-janet) and [reverse engineered the logic that generated them.](https://www.propublica.org/article/message-machine-starts-providing-answers)\n",
    "\n",
    "\n",
    "Let's load up the data and see how such a thing comes into existence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv,set_option, crosstab\n",
    "set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary = read_csv(\"https://github.com/computationaljournalism/columbia2020/raw/master/data/primary.csv\")\n",
    "primary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the point this graphic was published Montana, Oregon, South Dakota, Indiana, Kentucky, West Virginia, Pennsylvania, and South Carolina had yet to have their primaries. In all, there should be 2,261 counties for analysis. Here's where NY state had landed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary[primary[\"state_postal\"]==\"NY\"][[\"county_name\",\"winner\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is how the various counties across the country voted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary[\"winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, given all these potential predictor variables, which ones \"explain\" county-level voting patterns? Suppose, for example, we start simply, and consider whether or not a majority of the county voted for Bush in 2004. The outcome in 2004 and 2008 are both categorical variables so we can use `crosstabs` again to decide if the two \"co-relate\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab(primary[\"winner\"],primary[\"pres04winner\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this table tell us about the behavior of counties in 04 and its relationship to what happened in 08? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "290/(290+163)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1047/(1047+740)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, Obama won about 64% of those counties not voting for Bush in 04, while Clinton won about 59% of those counties that did vote for Bush in 04. Now, consider the **simple prediction rule**: If a county voted for Bush in 04, we’ll say that they will vote for Clinton in the primary, while if a county was mostly in favor of Kerry in 04, we’ll assign the win to Obama. \n",
    "\n",
    "Of course this rule isn’t perfect; by applying it, we would make 163 + 740 =\n",
    "903 mistakes (out of 2,261 counties, or about 40% error); we refer to these mistakes as having been \"misclassified\" by our simple rule. So the question becomes, can we do any better? What do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(163 + 740)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(163 + 740)/2261"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There might be better indicators of Obama’s success besides the vote in 2004 -- but how do we find them? Try a couple more categorical variables and tell me what you find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a couple of your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decision tree encodes a large search across the complete data set for predictors that are informative. \n",
    "\n",
    "Consider the top of the tree, \"the root\". Decision trees work by repeatedly splitting the data into two parts; the root or first \"split\" is the single division of the data into two pieces that produces the lowest misclassification error (well, it's a little more complicated, but this will do).\n",
    "\n",
    "To investigate this a little further, let’s consider the predictor that represents the\n",
    "percentage of a county that is African American; we now choose a breakpoint that divides the data into two pieces (those counties with a greater percentage of African Americans and those with a smaller percentage). We then form a table (as we did for counties that went for Bush or Kerry in 2004) and count the misclassification rate.\n",
    "\n",
    "Suppose we take 10% as the cutoff. We get the following table. How many errors do we make? What is the misclassification rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crosstab(primary[\"winner\"],primary[\"black06pct\"]< 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error rate?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this a bit less hands-on, how can we compute the error rate from the entries of the table directly? Notice that the errors are a function of each column, named `True` and `False` (yes, you can use booleans as names for columns). Then, we created a rule that predicts everyone in column `True` (their precint was less than 10% African American), say, as going for Clinton. We did this because among this category of counties, she won more than Obama. Her wins correspond to the `max()` counts in the column `True`. \n",
    "\n",
    "If we always assign the victor according to the candidate who won the most counties in the column `True`, then our mistakes will always be the `min()` counts in the column `True`. Get it? Let's test it out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = crosstab(primary[\"winner\"],primary[\"black06pct\"]< 0.1)\n",
    "predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(predict[False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(predict[True])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summing these two numbers gives you the total number of mistakes we would make if we had a prediction rule that said counties that are less than 10% African Amerian vote for Clinton and those that have a larger percentage will go for Obama. Get it?\n",
    "\n",
    "Now, the 20% figure at the root of the tree was obtained by finding the magic point that minimizes the misclassification errors. In fact, the search was conducted over all the variables in the data set and all the possible splits; and this choice produced the smallest error.\n",
    "\n",
    "Once this node has been chosen, we work our way down the tree, conducting the same search but on the specified subsets of the data, at each step attempting to minimize our errors. \n",
    "\n",
    "For quantitative variables, we have a choice of split points, and so the algorithm goes through a search. Here we vary a cutoff for the percentage of African Americans living in a county from essentially 0 to 0.5. For each choice of cutoff, we look at the associated error. \n",
    "\n",
    "What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fractions = [i/1000 for i in range(1,500)]\n",
    "error = []\n",
    "\n",
    "for f in fractions:\n",
    "    \n",
    "    predict = crosstab(primary[\"winner\"],primary[\"black06pct\"]< f)\n",
    "\n",
    "    # right branch\n",
    "    error_right = min(predict[True])\n",
    "    \n",
    "    # left branch\n",
    "    error_left = min(predict[False])\n",
    "    \n",
    "    error.append(error_left+error_right)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from plotly.express import line\n",
    "\n",
    "fig = line(x=fractions,y=error)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try another variable. How about the percentage of the vote in a county that was for Bush in 2004? The column is called `Bush04`. Let's see what that range is across the counties in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary[\"Bush04\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So among all the counties in the data set, the lowest vote percentage for Bush was 9% and the highest was 92%. So let's create a plot like the one above, but now we want our fractions to go from say .1 to .9, or 10-90%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and mkae a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, you should see that the lowest error rate we can come up with would classify a county as favoring Obama if the vote percentage for Bush in 2004 was 58% or less. All other counties would be predicted as voting for Clinton. Using this rule, we make 850 mistakes and this is as good as we can do with this predictor variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " crosstab(primary[\"winner\"],primary[\"Bush04\"]< 0.58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, look at the data and try another variable or two, either discrete or continuous (so a table or a curve) and tell me what the error rate is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is essentially what the decision tree process does. Given a set of counties and measurement on those counties, it searches for the best *split* of the data into two parts, trying to find the best prediction rule. Once we find the best we then treat the two parts separately. We look at the data under `True`, say, and look through all the variables and try to divide it into two pieces. We then look at the data left over under `False` and find its best predictor. And we just keep going.\n",
    "\n",
    "This is an example of  a *greedy algorithm*, meaning at each step, it is looking for the best way to divide the data into two pieces. There are things we can do to speed it up computationally, and there are issues with really trying to minimize misclassification rates, but this is essentially the approach.\n",
    "\n",
    "Stop and think what this simple process has produced for us; we have a very intuitive structure (something akin to the game 20 questions) that makes evident \"important\" variables that help \"explain\" voting patterns. This kind of tool lives somewhere between data analysis and modeling; it is technically a model all by itself (making predictions) but is often used as a way to identify important predictors for another stage of model.\n",
    "\n",
    "This decision tree is part of a large class of methods called CART for Classification and Regression Trees and was developed in the 1980s as part of a move to deal with bigger and meaner data sets.\n",
    "\n",
    "**Fitting a tree**\n",
    "\n",
    "We will use a package called Scikit Learn to do the actual tree growing. Amanda Cox, having a Master's degree in Statistics, used R and so our fits and process will be a little different than hers. First, we need to do something about the missing values. Scikit Learn is a little weak in what it's capable of, but it's usable. It is possible to work around NA's in Brieman's algorithm, but for now we have to drop any row that has a missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "primary.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we are down about 40 counties. Now, let's load up what columns we want to model with, and create a data set consisting of only those predictor variables (also called *features* or *independent variables* in a learning problem). We will also create a single column for the variable we'd like to predict (also called the *dependent variable*). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "# list out the columns we want to include (potentially) in our model.\n",
    "# the tree algorithm will search over these as we searched for the \"best\" predictors.\n",
    "features = [\"pres04margin\",\"black06pct\",\"hisp06pct\",\"white06pct\",\"growth\",\n",
    "            \"pct_less_30k\",\"pct_more_100k\",\"pct_hs_grad\",\"pct_homeowner\",\"POP05_SQMI\"]\n",
    "\n",
    "# create a response variable and the data frame of predictors \n",
    "y = list(primary[\"winner\"])\n",
    "X = primary[features]\n",
    "\n",
    "# create a tree fitting machine that will only split down to a \"depth\" of 3\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "\n",
    "# and use the machine make the fit with our data.\n",
    "tree = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a look. It won't be as nice as Amanda's but it will help us explore the relationships between variables in our data set. Remember trees are A+ on interpretability. If you haven't, first install a package that will draw our trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And draw the tree! This is largely readable (feature names, class names, etc) and otherwise is a kind of copy-and-repeat bit of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from pydotplus import graph_from_dot_data\n",
    "\n",
    "dot_data = export_graphviz(tree, out_file=None, \n",
    "                         feature_names=features,\n",
    "                         class_names=[\"clinton\",\"obama\"],  \n",
    "                         filled=True, rounded=True)  \n",
    "graph = graph_from_dot_data(dot_data)  \n",
    "\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s think a bit more about the tree-growing process; with each split, we cut\n",
    "our data at the node into two pieces so that the “sample size” at each of the child nodes is lower than its parents. We then represent the data in the leaves with a simple model; for our 0/1 data (Obama or Clinton), we classify leaves according to majority vote.\n",
    "\n",
    "In principle we can grow trees until there’s a single entry in each node -- What\n",
    "might be the problem with that? How do we decide to stop splitting? When we\n",
    "run out of data? \n",
    "\n",
    "The algorithm Brieman developed grows a big tree and then *prunes it back*. So take the tree we have above, and you can see that most of the right side of the tree remains for Obama, no matter how far down we split. That means that the Clinton counties remain mistakes. We've added a lot of complexity to the tree, but really, stopping at the first split is all we need. We can prune back splits that add more complexity than they're worth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![tree](https://static01.nyt.com/images/2008/04/16/us/0416-nat-subOBAMA.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the R display that Amanda based her graphic on (unlike the SciKit Learn output) the heights of the branches correspond to the error in represented by the model. So, the at the root, we are dealing with all the counties -- 1210 went for Clinton\n",
    "and 1031 for Obama -- therefore since Clinton won more counties, we would\n",
    "predict all future counties for Clinton, making 1030 mistakes.\n",
    "\n",
    "The first split on the percentage of the county that is\n",
    "African American brought us down to 700 errors, a big drop; the next division\n",
    "based on education is another big drop, about half the size. As we continue to refine the tree, however, the improvements diminish.\n",
    "\n",
    "In the mid-1980s a fair bit of theoretical and methodological work was devoted\n",
    "to understanding the behavior of this kind of algorithm; we are using it as a bit\n",
    "of data analysis (how does a “response” relate to the potential “explanatory”\n",
    "variables?) but it can also be used as a tool for making predictions.\n",
    "\n",
    "A lovely graphical introduction to decision trees is [given here.](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
    "\n",
    "**More recent data**\n",
    "\n",
    "Here is a data set collected by Loren Collingwood, a political scientist. It accumulates many of the same county-level variables but is looking at the recent national election instead. Let's try out our new learning skills with SciKit Learn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv, set_option\n",
    "data = read_csv(\"http://www.collingwoodresearch.com/uploads/8/3/6/0/8360930/county_data.csv\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_option(\"display.max.columns\",100)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the data from the NYT, we don't have a `winner` specified -- we only have the percentage earned by each candidate. So we will use a list comprehension to create a column called `winner` giving the county to Clinton if she had a greater percentage of the vote than Trump."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"winner\"] = ['Trump' if data[\"pct_clinton\"][i] < data[\"pct_trump\"][i] else 'Clinton' for i in range(data.shape[0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that it's there (scroll way over to the right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"winner\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, well, there we are. So let's fit the tree and see what we find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "features = [\"per_capita_income\",\"pobama12cnty\",\"percent_white\"]\n",
    "\n",
    "y = list(data[\"winner\"])\n",
    "X = data[features]\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=3)\n",
    "tree = dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image  \n",
    "from pydotplus import graph_from_dot_data\n",
    "\n",
    "dot_data = export_graphviz(tree, out_file=None, \n",
    "                         feature_names=features,\n",
    "                         class_names=[\"Clinton\",\"Trump\"],  \n",
    "                         filled=True, rounded=True)  \n",
    "graph = graph_from_dot_data(dot_data)  \n",
    "\n",
    "Image(graph.create_png()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was just a toy example. There are so many variables and in principle we can put all of them into the mix and see what happens. Collingwood didn't have a dictionary for his data, but he was able to answer a few questions. Here are the columns I couldn't read easily and his explanations.\n",
    "\n",
    ">totpop1014cnty what does 1014 refer to? 1014 IS SAMPLED BETWEEN YEARS 2010-14 FOR ACES <br>\n",
    "ppiwht1014cnty ppi? --I'M PRETTY SURE THIS IS PERCENT WHITE, THN PERCENT BLACK, ETC. <br>\n",
    "ppiblk1014cnty <br>\n",
    "ppihisp1014cnty <br>\n",
    "pimm1014cnty mm? PERCENT IMMIGRANT <br>\n",
    "ppi0014cnty ppi again and what is 0014? THE 0014 WOULD NOW BE A CHANGE MEASURE -- PERCENT CHANGE FROM 2000 TO 2014. <br>\n",
    "psocsec0014cnty <br>\n",
    "ownthirty0014cnty ownthirty? <br>\n",
    "ownfifty0014cnty <br>\n",
    "pmanufact1014cnty ? employment in manufacturing? CORRECT -- THIS WAS PREDICTOR OF TRUMP VOTE <br>\n",
    "pct65over00cnty this is in 2000? YES, FOR OLD PEOPLE <br>\n",
    "medoohvalue00cnty ? MEDIAN HOME VALUE 2000 <br>\n",
    "pctcomlong00cnty comlong? I THINK THIS IS COMMUTE OVER 30 MINS -- BUT I DIDN'T USE THIS SO NOT 100% ON THAT. <br>\n",
    "pctdiffh95_00cnty ? NOT SURE <br>\n",
    "pctfb00cnty ? FOREIGN BORN <br>\n",
    "pcpi00cnty pcpi? NOT SURE <br>\n",
    "Rpt. ? NOT SURE <br>\n",
    "state.y STATE NAME/CODE. PROBABLY A state.x AS WELL? FUNCTION OF MERGING IN R <br>\n",
    "pct_change_R ? <br>\n",
    "manu_loss ? IS THIS A DUMMY? IF SO, DID THE COUNTY RECEIVE MANUFACTURING LOSS IF YES = 1, ELSE = 0 <br>\n",
    "rural ? RURAL COUNTY BASED OFF OFFICIAL CENSUS DESIGNATION (DUMMY) <br>\n",
    "rural_150 ? SIMILAR MEASURE, THIS IS THE ONE I THINK IS BASED OF 100 PER SQ/MILE. BUT WOULD HAVE TO CHECK<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
